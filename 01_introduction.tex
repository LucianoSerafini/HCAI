\section{Introduction}

% \paragraph{Agent-Environment pair} 

\lo{I would mention humans immediately in the reference framework, otherwise the human-centered part of the concept will not be clear. What about an agent-human-environment framework? in which human is at the center?}

\as{I agree.  If we propose "foundations for human-centered AI" we should first say what we mean by "human-centerd AI", and then explain why foundations are needed for it.}

\jr{In the definition we can cite the HLEG AI guidelines of Trustworthy AI, https://ec.europa.eu/futurium/en/ai-alliance-consultation/guidelines}

We consider a reference framework composed of an \jr{HCAI} agent (or simply agent) that operates in an
environment \lo{"in presence of humans"...}. We call this framework the agent-environment framework. In an
agent-environment framework there is a clear division between what is
internal to the agent, and what is external to the agent, i.e., the
environment. The agent \jr{observes} the environment via sensors, and \jr{acts upon} 
modifies the environment via \jr{effectors}. Every interaction between the agent
and the environment happens through these two categories. \jr{Observations and actions}
are considered in a very broad sense \todo{observation or percept, not sensors, are the input concepts corresponding to action as output concept. Sensor is the input concept corresponding to the effector input concept}. Sensors range from low level
sensors to input in natural language, images, movies, etc.  Actions
may range from physical actions, as moving ahead, moving an arm,
grasping an object, to communicative actions, such as jr{displaying} some data
on the screen, uttering a sentence, smiling, or playing a song or a
movie. In general
there is no synchronisation between actions and observations, and both
actions and observations can happen independently without following a
precise protocol.  Finally the environment may change due the the effect
of some agent's action, or it may change due to some external cause,
which is not controlled by the agent (e.g., \jr{a human} agent can
operate in the environment) \todo{I took out "some other agent" since the abstract mentions that we restrict the scope of our paper to single agent systems. BTW should we and why? :)}.

% \paragraph{Goal directed agents} 
An AI agent always has one or more goals to achieve\footnote{\jr{The so-called \emph{reflex} agents \cite{aima4}, which internal architecture reduces to a single reasoning component that directly maps observations to actions do not internally maintain an explicit representation of their goals. Even in this case, the mapping function or rule base that implements their reflexes are nevertheless programmed or machine learned with some implicit goal in mind for these agents to achieve.}}. At a given
time-point of its life, some goals can be completely
achieved (and therefore these are not goals anymore), some of them can be
partially achieved, and for other the agent does not have a plan to
achieve them. In order to
achieve its goal(s) an \jr{HCAI} agent can autonomously execute actions \todo{too much content in parentheses, hard to read} (e.g.,
an autonomous agent with a planner obtains these actions by planning)
or some external event may happen so that the agent will get closer to
the goal \todo{Jacques: I do not think this is the best example of "external event" bringing the agent closer to its goal; a simpler, more human-centric example would be \emph{a human collaborating with the agent carries out an action that bring them both closer to their common goal.} (e.g., a classifier is trained with new labelled data)}.
Goals in agents exist on several levels: (i) beneficial properties of
a model and its reasoning on the level of model schema (including
algorithm) design, (ii) high performance (correctness and/or
efficiency) on the level of setting concrete parameters of a model in
an agent, and (iii) goals that are represented within the agent and
direct the agent's behaviour in the world.  Typical machine
learning goals are on level (ii) and typical planning goals are on
level (iii).  However, the agent might have the goal to improve itself
by finding additional data for training (as done in Active Learning)
or by attempting modifications of its own system (similar to genetic
programming) where the levels merge or are all represented within the
agent.

% \begin{tikzpicture}[every node/.style = {scale=.6}]
% \draw (0,0) rectangle (5,3);
% \draw (0,1) -- (4,1) -- (4,2) -- (1,2);
% \node[circle,draw,fill=black!10] (A) at (.5,.5) {A};
% \node[circle,dashed,draw,red,fill=red!10] (G) at (2.5,1.5) {G};
% \foreach \i/\p/\q in {1/1.5/0.5, 
%                      2/2.5/0.5, 
%                      3/3.5/0.5, 
%                      4/4.5/0.5, 
%                      5/4.5/1.5,
%                      6/4.5/2.5,
%                      7/3.5/2.5,
%                      8/2.5/2.5,
%                      9/1.5/2.5,
%                      10/0.5/2.5,
%                      11/0.5/1.5,
%                      12/1.5/1.5}{
% \node[circle,dashed,draw] (\i) at (\p,\q) {A};
% };
% \foreach \i/\j/\a in {A/1/east,
%  1/2/east,2/3/east,3/4/east,4/5/north,5/6/north,6/7/west,7/8/west,8/9/west,9/10/west,10/11/south,11/12/east,12/G/east}
% \draw[->,dashed] (\i) -- node[sloped,above]{\a} (\j);
% \end{tikzpicture}

% \begin{tikzpicture}
% \draw (0,5) -- (0,0) -- (5,0);
% \foreach \i/\n in {0/0.0,1/0.2,2/0.4,3/0.6,4/0.8,5/1.0}{
%    \draw[line width=.1] (-.2,\i) -- (.2,\i);
%    \node at (-.4,\i) {\n};};
% \foreach \i/\n in {0/0.0,1/0.2,2/0.4,3/0.6,4/0.8,5/1.0}{
%    \draw[line width=.1] (\i,-.2) -- (\i,.2);
%    \node at (\i,-.4) {\n};};
% \node[circle,draw,fill=black!10] (C) at (1,1) {C};
% \node[circle,dashed,draw,red,fill=red!10] (G) at (5,5) {C};
% \foreach \i/\p/\q in {1/2/3,
%                      2/4/3.5}{
% \node[circle,dashed,draw] (\i) at (\p,\q) {A};
% };
% \foreach \i/\j in {C/1,1/2,2/G}
% \draw[->,dashed] (\i) -- node[sloped,above]{train} (\j);
% \node[rotate=90] at (-1,2.5) {recall};
% \node at (2.5,-1) {precision};
% \end{tikzpicture}


% \paragraph{Human Centered AI agent} 
\todo{distinguish between the different human roles} 
A way to see human-centered AI is shown in
figure~\ref{fig:hcai-onion}.  An initial observation concerns the fact
that humans cannot be separated from the environment where they
operate. Indeed they are \emph{part} of the environment; they are
\emph{embedded} in it, In other words, humans and environment are
complementary, interconnected, and interdependent in the natural
world, and they interrelate to one another. Therefore Human-Centered Artificial
Intelligence should take into account both humans and the
environment where humans operate.

\begin{figure}[h]
  \begin{center}
    \input{onion}
\end{center}
\caption{\label{fig:hcai-onion} The Human-Centered Artificial
  Intelligence \cn{I think that we will be required to explain better the figure with a more ``extensive" caption, as well as define human (H) and environment (E). We could change it like that: ``A simple schematization of Human-Centered Artificial Intelligence represented as an set of agents acting as an integrated artificial intelligent systems (external ring) and their interactions (inner ring) with respect to humans (H) and environment (E)"}}
\end{figure}

The external ring in Figure~\ref{fig:hcai-onion} represents the
artificial intelligent agent(s). It can refer to one single
intelligent agent or a set of interrelated intelligent agents, which
autonomously interact following, e.g., a multi-agent paradigm. Every
single artificial intelligent system can implement one (or more) tasks
using one (or more) specific approaches/models. For instance an
``artificial reasoner'' could represent its knowledge in some logical
formalism and support query answering and inference through automatic
reasoning (e.g., SAT, ASP); an ``artificial classifier'' could be
implemented in a deep neural network that is capable to classify
images into different classes. An ``artificial planner'' can produce
plans and take decision exploiting classical planning techniques or
reinforcement learning. To solve complex tasks, different systems and
methodologies should be integrated. A black box integration of each
``intelligent agent'' is not sufficient; it is necessary to integrate
and make all these different approaches to collaborate one another in
a glass-box method. 

Finally, the intermediate ring represents the interaction between the
human/environment and the integrated artificial intelligent system.
The type of interactions that one can see between humans/environment and
artificial agents happen across a set of artifacts that are
``shared'' by the humans/environments and the artificial system.
We briefly describe them here, but we will describe more extensively
in the rest of the document:
\emph{Dependability Requirements} are
artifacts, produced by humans, that specify the expected behaviour and
some other non-functional properties of the artificial intelligent
system. Some of them are usually specified in a formal language (that
has an intuitive semantics for the human) so that it is possible to
verify automatically that the artificial intelligent system behaves
according to the requirement to a certain certainty degree.  Some
of them are not expressed in a formal language and how they can be
represented in a mathematical structure is an open issue.
\emph{Actions} are considered in a broader sense. They
  represent the actions that the artificial agent can perform as well
  as the action that the human can perform in the direction of the
  artificial agent. They can be physical actions (that has effects on
  the environment) or informative actions (that have effects on the
  knowledge of the human or of the artificial agent). 
\emph{Observations} are all the data that the artificial
  intelligent system can collect through its sensors.
\emph{Explanations:} are artifacts that are produced by an artificial
  agent that ``explains'' to the human the reason of its
  behaviour. E.g., the reason why it took an action or a
  decision. Explanations should be human understandable and acceptable
  in a rational system shared by the machine and the human. 


