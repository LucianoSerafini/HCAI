\section{Key components of a HCAI } 

\subsection{Observations}

% Comments by Sonja:

% This section describes sensors, but mostly sticking to textual examples
% (NLP). There are thousands of types of sensors, and they all serve their
% purpose depending on what data they sense. This is different for all
% pilots - IoT uses variety of sensors: cameras, ultrasonic, infrared,
% etc.; Robotics use potentiometers, accelerometers, Gimbles etc.; Media
% uses cameras.... Perhaps the examples for sensors in this section can be
% specific for each pilot (maybe even include this section in each
% pilot).

% What is the meaning of "sensor of a "text classifier""?  

% Sensor is something that gives information - if this here also stores
% a file, than that is a transducer - does something.

An agent can observe the humans and the environment through its
sensors. Here we mean sensor in a broad sense, i.e., everything that
can be used by the agent to autonomously acquire data about the
environment (including the humans) and everything the humans and the
environment can use to syncronously or asyncronouysy input data to the
artificial intelligent agent.

Observations are collected via
sensors. There are thousands of types of sensors, and they all serve
their purpose depending on what part of the environment they
sense. Every single pilot in the project has defined a different set
of sensors. E.g., IoT sensors are: cameras, ultrasonic, infrared,
\dots; Robotics sensors are: potentiometers, accelerometers, Gimbles
\dots.; Media sensors include: digital cameras, audio recording,
textual documents, etc.  Essentially, sensor systems are those that
inject input data. Input data can be represented in many different
ways, they can be continuous discrete, structured, lists, trees,
etc. Typically, input data is provided with some meta information that
describes its structure.  Examples of input data are video, picture,
all type of sensors, texts, spoken sentences, etc. In online system,
input data are streams of data, not just a batch of data. Among the
observations we also include the ``commands/requests'' that the human
poses to the artificial agent.  For this reason, one can think that in
general every data is associated with some time stamp $t$ (that can be
relative or absolute). In some cases time information is not relevant
for the decision process of the agent, in other it is. However, what
it should always hold is that agents can take a decision at time $t$
only on the basis of the data that are available to the agent before
this time, i.e., data that are tagged with the timestamp $t'< t$.

\def\observations{\mathbf{Obs}}
\def\observation{Obs}

\begin{definition}
The \emph{observations} of an HCAI agent are all the data that an agent
can observe at any possible instant of its evolution. The observations
available to the agent at time $t$ are $\observations_{\leq t}=\{\observation_{t'}\}_{t_0\leq t'\leq t}$,
where $\observation_t$ denotes the data provided by the agent's sensors at
time $t$. 
\end{definition}

$\observation_t$ can or cannot be a function of the state of the environment
and the agent at time $t$. The most general formulation can be that
$\observation_t\sim Pr(\cdot\mid\environmentState_t,\agentState_t)$,
i.e., that the observations at time $t$ follows some (unknown)
probability distribution conditioned by the state of the environment and of the
agent at time $t$. 


\def\actions{\mathcal{A}}
\subsection{Actions}

The set of actions of an agent are all the possible way that the agent
has to affect the state of the environemnt and the humans that
iteracts with it. Actions can be physical actions, e.g., the robot is
moving 10 cm forward, an autonomous car braking, an autonomous
personal assistant books a room in hotel Bellavista, or buys 100
stocks of the company SuperGulp.  Another type of action are
\emph{informative actions} that inform the human of a certain
conclusion that the agent has reached. Examples of informative actions
are the communication of the result of a classifications of an iten
provided in input.

Actions should produce a tangible effect on humans or on the
environment. This means that if their execution dont fail, the state
of the environment once the action is terminated should be different
from the state of the environment before the execution of the
actions. The oucome of an action in general is not deterministic,

Notice that a pure internal agent reasoning (e.g., an agent
taking a decision about something) should not be considered as an
action, becase it changes only the internal state of the agent, but
does not affect the environment. The real action consists on the
effective execution of the decision.

Concrete examples of actions are: If an agent task is to classify
documents in $n$ classes its set of actions are those that
\emph{communicates} the result of the classification of an item into
one or more class. This action will change the knowledge state of the
human that has requested such a classification.  A second example of
actions of a robot that navigates in a room which are
$\{forward(x),turn(\theta) \mid x \in R^+, \theta \in [-\pi,\pi]\}$.
As one can see from the above example the set of actions associated to
an artificial agent can be discrete or continuous and they strictly
depend from the input. Notice that in the set of agent's actions do
not includes the actions that are executed by the humans and by the
other events that can asynchronously happen in the environment.  An
important aspect of the actions in HCAI is that they should be
relevant for the humans that populates the environment.

\def\effects{\mathbf{Eff}}
\begin{definition}
  The set of actions of an HCAI agent is a set
  $\actions$ such that for each $a\in\actions$ one has associated a
  set of effects $\effects(a)$ which describes (non
  deterministic/probabilistic) transformation of the state of the
  environment which involves a change of the state of the humans that
  lives in the environment. 
\end{definition}

\as{This is a very important point, that differentiates foundations
  for "human-centered" AI from others. The previous examples of
  actions only depend on the actor.  Here, you say that actions depend
  only on the human, or "receiver".  This definition seems to entails
  that actions in a HCAI system are relations between the (AI) actor
  and a (human) counterpart: it's a bit like affordances.
  Interesting, but tricky\ldots\ And not really what Def~2 below is
  saying.}
\ls{I don't know precisely how to address the above important point,
  perhaps we can discuss this in the next meeting.} 


% For instance in a ranking system, the user might be interested only in
% the first 10 documents. In this case, though the outcome of the system
% is an entire ranking of all the documents, the actions which are
% relevant for the human are only the first 10 documents in each
% rank. Another example with physical robots is the following: the
% outcome of the planner of a robot could be to move from the kitchen to
% the bedroom following one particular path (path planning) though the
% human could be interested only in the starting and ending point of the
% path, and not in the precise trajectory that is followed by the
% robot. Some outcomes of a robot could also not be perceivable by a
% human. E.g., the fact that the personal assistance books the hotel
% BellaVista through Trivago or Booking.com is not perceived by the
% user, which only gets the booking information as outcome.


\subsection{Explanations} 
An explanation is an explanation of some decision. In particular, in
HCAI explanations explains to a (set of) human the decision of
executing an action $a$, made by an agent at time $t$ when it was in
the state $\agentState_t$. However, as argued in
\cite{miller2019explanation}, the definition of explanations for a
decision taken by a HCAI agent cannot be provided without making
explicit reference to whom the explanation is addressed, i.e, the
humans. aka \emph{the explanee}. 
In general an explanation of a decision taken by the agent can be
independent from how the agent reaches such a decision. This is
especially true when the decision is taken by a black-box method such
as a neural network. 

\begin{definition}[Explanation]
  The explanation of a decision to execute the action $a$ done by an
  agent in the state $\agentState_i$. Such an explanation should be
  understandable by the explanee.
\end{definition}

\ls{Maybe we want to add some more details here abou the nature of
  explanations, eg., reference to causal models, \dots. } 

\subsection{Requirements}
A requirement, including the concept of dependability requirement, is the formulation of a functional need that an  artificially intelligent
system must satisfy. This is central to the problem of verifiable AI, which has the
objective of checking that the systems meets its
requirements, including the functional specification and the dependability attributes. 
Dependability requirements are expressed in a human understandable
way, they are indeed formulated by humans. Then the problem of
verifiable AI, is how to translate these requirements in algorithms
that check that the system is compliant with them. 
Being a human produced artifact, providing a sharp definition of
(dependability) requirement is rather complex if not impossible. In
the examples provided in this overview document (e.g., automotive and
medical domain) dependability requirements can be classified into two
categories 
\begin{description}
\item[Input-output constraints for inference:] given a certain (class of) input data
  a specific (class of) output data is required. If input and output
  data classes are described in some logical term then this type of
  requirement can be seen as pairs: 
$$
\Delta\vdash\Gamma 
$$
where $\Delta=\{\delta_1,\dots,\delta_m\}$ is a set of properties of
the input, and $\Gamma=\{\gamma_1,\dots,\gamma_n\}$ is a set of
properties of the expected output. Intuitively $ \Delta\vdash\Gamma $
means that ``if the input meets all the properties in $\Delta$, then
the output should meet at least one property in $\Gamma$.''.  This
input output constraint on inference can also be associated with some
level of satisfiability. E.g., the constraint $\Gamma\vdash\Delta$
should be satisfied at least 99\% of the time.

\item[Input-output constraints for learning:] one of the main
  characteristics of AI systems is that their behaviour is determined
  by the data that are used to train the system. Examples of
  requirement in this sense is the fact that the training data are
  representative of a given population for which the AI system is
  designed. There is no simple way to express this type of requirement
  in some mathematical structure. This could be the subject of further
  study in the project. 
\end{description}

\todo{Introduce non-determinism as a source of verification challenges.}% single-agent (the AI agent) and non-deterministic environments, sensors and actuators. This is what we have also in the “onion”, i.e., observations (produced by sensors) and actions (Produced by actuators); if humans are taken into account. They act independently from the artificial agent, and therefore they are a source of indeterminism in the environment. Explain how this leads us into safety and liveness properties, and notions of fairness; temporal logics may be applied as a means to specify such requirements.

% NOTE: We may need a model of the environment and, possibly, a model of the human. Notice that there is non-determinism so the actions of our agent may have differing outcomes. However, modelling a human is very difficult so we need to make very loose assumptions with this respect.

\subsection{Artificial intelligent models} 
An AI agent takes decisions on how to act in the environment or how to
react to some input given by the humans and/or by the environment, on
the basis of one or more \emph{models} of the surrounding environment
and of the humans it is currently interacting with. Models are
abstract (computational) structures that allow to answer queries about
what holds in the current or in the past situation and to predict what
will be true/false in the future. Models can also be used to simulate
possible evolutions of the humans-environment in order to take the
``right'' decision now.

Artificial intelligent agents are equipped with a set of artificial
intelligence models that represent the knowledge of the agent about
the environment. Such a knowledge is used to support the agent in
making decisions about which action to perform. In general we cannot
assume that such models are \emph{correct}, i.e., that their
predictions are effectively true.  For this reason it is more
appropriate to speak about \emph{belief} instead of knowledge Neither
we can assume that models are \emph{complete} i.e., that they describe
the environment in all it's details. Indeed they are
\emph{simplified/abstract} representations of some aspect of the
environment obtained by abstracting away irrelevant (believe to be
irrelevant) details.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
